{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Savercat with all genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, LeakyReLU, Lambda\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\n",
      "1.5.1\n",
      "scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.20.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.1\n"
     ]
    }
   ],
   "source": [
    "base_name = os.path.basename(os.getcwd())\n",
    "print(base_name)\n",
    "print(sc.__version__)\n",
    "sc.settings.verbosity = 3  \n",
    "sc.logging.print_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 29259 × 20042\n",
      "    obs: 'Cycle', 'patient'\n"
     ]
    }
   ],
   "source": [
    "adata = sc.read_h5ad('../data/adata_subsample_allg.h5ad')\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Savercat preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils functions in utils_0509.py\n",
    "from utils_0509 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_key = 'Cycle' # the name of the cell-level label to be predicted\n",
    "batch_key = 'patient' # the name of the cell-level label to be adjusted for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizing by total count per cell\n",
      "    finished (0:00:24): normalized adata.X and added    'n_counts', counts per cell before normalization (adata.obs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 29259 × 20042\n",
       "    obs: 'Cycle', 'patient', 'n_counts', 'size_factors'\n",
       "    var: 'mean', 'std'\n",
       "    uns: 'log1p'\n",
       "    obsm: 'saver_targetL', 'B_raw', 'B', 'loglib', 'saver_batch'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# savercat preprocessing step\n",
    "adata = savercat_preprocess(adata, predict_key=predict_key, adjust_key=batch_key)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import network buiding functions in network_0509.py\n",
    "from network_0509 import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if train on highly variable genes, then keep enc=(256, 256, 128), dec=(128, 256, 256)\n",
    "# leave all the parameters unchanged\n",
    "SAVER_net = CVAE(x_input_size = adata.n_vars, # number of genes\n",
    "                 b_input_size = adata.obsm['saver_batch'].shape[1], # number of batches including lib-size\n",
    "                 lb_input_size = adata.obsm['saver_targetL'].shape[1], # number of labels to predict\n",
    "                 enc = (512, 256, 128), # dim of the encoder\n",
    "                 dec = (128, 256, 512), # dim of the decoder\n",
    "                 latent_k = 30) # dimension of the low-dimensional latent space\n",
    "SAVER_net.build()\n",
    "SAVER_net.compile_model(pred_weight=1, kl_weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "412/412 [==============================] - 156s 378ms/step - loss: 4642.0742 - pred_loss: 34.5318 - kl_loss: 15.1021 - recon_loss: 4592.4136 - val_loss: 4470.5635 - val_pred_loss: 20.6944 - val_kl_loss: 24.1963 - val_recon_loss: 4425.5928\n",
      "Epoch 2/300\n",
      "412/412 [==============================] - ETA: 0s - loss: 4436.8481 - pred_loss: 9.9782 - kl_loss: 15.2816 - recon_loss: 4411.5889\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "412/412 [==============================] - 171s 415ms/step - loss: 4436.8481 - pred_loss: 9.9782 - kl_loss: 15.2816 - recon_loss: 4411.5889 - val_loss: 4445.1436 - val_pred_loss: 22.6472 - val_kl_loss: 17.3184 - val_recon_loss: 4405.0869\n",
      "Epoch 3/300\n",
      "412/412 [==============================] - 157s 380ms/step - loss: 4405.1743 - pred_loss: 4.0413 - kl_loss: 15.3237 - recon_loss: 4385.8091 - val_loss: 4421.0083 - val_pred_loss: 16.3899 - val_kl_loss: 14.7778 - val_recon_loss: 4389.8013\n",
      "Epoch 4/300\n",
      "412/412 [==============================] - ETA: 0s - loss: 4396.3667 - pred_loss: 2.0084 - kl_loss: 15.4010 - recon_loss: 4378.9551\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "412/412 [==============================] - 138s 335ms/step - loss: 4396.3667 - pred_loss: 2.0084 - kl_loss: 15.4010 - recon_loss: 4378.9551 - val_loss: 4418.8760 - val_pred_loss: 17.2216 - val_kl_loss: 14.7994 - val_recon_loss: 4386.8149\n",
      "Epoch 5/300\n",
      "412/412 [==============================] - ETA: 0s - loss: 4391.3936 - pred_loss: 1.4661 - kl_loss: 15.4381 - recon_loss: 4374.4863\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "412/412 [==============================] - 132s 321ms/step - loss: 4391.3936 - pred_loss: 1.4661 - kl_loss: 15.4381 - recon_loss: 4374.4863 - val_loss: 4417.5449 - val_pred_loss: 17.3802 - val_kl_loss: 14.7019 - val_recon_loss: 4385.4233\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# no need to modify this block\n",
    "# label guided initialization step\n",
    "loss = SAVER_net.model_initialize(adata, fit_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the directory where you want to save the file\n",
    "# 'weights_step1.h5' is the file name\n",
    "SAVER_net.model.save_weights('../data/weights_init.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if train on highly variable genes, then keep enc=(256, 256, 128), dec=(128, 256, 256)\n",
    "# leave all the parameters unchanged\n",
    "# same as block 8 but use the weight you just saved\n",
    "SAVER_net = CVAE(x_input_size = adata.n_vars,\n",
    "                 b_input_size = adata.obsm['saver_batch'].shape[1],\n",
    "                 lb_input_size = adata.obsm['saver_targetL'].shape[1],\n",
    "                 enc = (512, 256, 128),\n",
    "                 dec = (128, 256, 512),\n",
    "                 latent_k = 30)\n",
    "SAVER_net.build()\n",
    "SAVER_net.load_weights('../data/weights_init.h5') # fill in the weight file you just saved\n",
    "SAVER_net.compile_model(pred_weight=0., kl_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "412/412 [==============================] - 140s 340ms/step - loss: 4405.4019 - pred_loss: 9.7829 - kl_loss: 15.4049 - recon_loss: 4389.9951 - val_loss: 4407.4219 - val_pred_loss: 34.8856 - val_kl_loss: 16.9893 - val_recon_loss: 4390.4316\n",
      "Epoch 2/300\n",
      "412/412 [==============================] - 134s 324ms/step - loss: 4393.3135 - pred_loss: 29.7949 - kl_loss: 15.5981 - recon_loss: 4377.7188 - val_loss: 4399.9731 - val_pred_loss: 42.3522 - val_kl_loss: 16.3093 - val_recon_loss: 4383.6631\n",
      "Epoch 3/300\n",
      "412/412 [==============================] - 147s 358ms/step - loss: 4384.5337 - pred_loss: 37.2475 - kl_loss: 15.8889 - recon_loss: 4368.6421 - val_loss: 4395.4683 - val_pred_loss: 45.5484 - val_kl_loss: 16.0384 - val_recon_loss: 4379.4312\n",
      "Epoch 4/300\n",
      "412/412 [==============================] - 146s 353ms/step - loss: 4378.2847 - pred_loss: 43.2584 - kl_loss: 16.0765 - recon_loss: 4362.2085 - val_loss: 4393.0918 - val_pred_loss: 45.3063 - val_kl_loss: 16.2164 - val_recon_loss: 4376.8755\n",
      "Epoch 5/300\n",
      "412/412 [==============================] - 137s 334ms/step - loss: 4373.1733 - pred_loss: 46.6507 - kl_loss: 16.3326 - recon_loss: 4356.8442 - val_loss: 4392.1621 - val_pred_loss: 48.7385 - val_kl_loss: 16.1997 - val_recon_loss: 4375.9624\n",
      "Epoch 6/300\n",
      "412/412 [==============================] - 134s 326ms/step - loss: 4368.1812 - pred_loss: 49.8155 - kl_loss: 16.5168 - recon_loss: 4351.6660 - val_loss: 4390.8140 - val_pred_loss: 53.0930 - val_kl_loss: 16.0893 - val_recon_loss: 4374.7246\n",
      "Epoch 7/300\n",
      "412/412 [==============================] - 135s 327ms/step - loss: 4364.0000 - pred_loss: 57.9500 - kl_loss: 16.6137 - recon_loss: 4347.3862 - val_loss: 4390.3394 - val_pred_loss: 61.0022 - val_kl_loss: 16.3188 - val_recon_loss: 4374.0200\n",
      "Epoch 8/300\n",
      "412/412 [==============================] - 135s 328ms/step - loss: 4360.0586 - pred_loss: 61.8717 - kl_loss: 16.7423 - recon_loss: 4343.3169 - val_loss: 4388.6113 - val_pred_loss: 62.8068 - val_kl_loss: 16.2478 - val_recon_loss: 4372.3638\n",
      "Epoch 9/300\n",
      "412/412 [==============================] - 135s 329ms/step - loss: 4356.4277 - pred_loss: 66.2411 - kl_loss: 16.8320 - recon_loss: 4339.5962 - val_loss: 4388.5850 - val_pred_loss: 70.2289 - val_kl_loss: 16.3546 - val_recon_loss: 4372.2300\n",
      "Epoch 10/300\n",
      "412/412 [==============================] - 136s 330ms/step - loss: 4352.7441 - pred_loss: 72.5944 - kl_loss: 17.0218 - recon_loss: 4335.7236 - val_loss: 4388.6479 - val_pred_loss: 75.2628 - val_kl_loss: 16.3771 - val_recon_loss: 4372.2700\n",
      "Epoch 11/300\n",
      "412/412 [==============================] - 137s 332ms/step - loss: 4349.5513 - pred_loss: 77.4018 - kl_loss: 17.1101 - recon_loss: 4332.4409 - val_loss: 4389.1558 - val_pred_loss: 78.0403 - val_kl_loss: 16.5014 - val_recon_loss: 4372.6538\n",
      "Epoch 12/300\n",
      "412/412 [==============================] - 138s 335ms/step - loss: 4346.2339 - pred_loss: 84.1260 - kl_loss: 17.3435 - recon_loss: 4328.8892 - val_loss: 4390.0244 - val_pred_loss: 85.6383 - val_kl_loss: 16.7543 - val_recon_loss: 4373.2705\n",
      "Epoch 13/300\n",
      "412/412 [==============================] - ETA: 0s - loss: 4343.3198 - pred_loss: 88.1691 - kl_loss: 17.6137 - recon_loss: 4325.7046\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "412/412 [==============================] - 144s 349ms/step - loss: 4343.3198 - pred_loss: 88.1691 - kl_loss: 17.6137 - recon_loss: 4325.7046 - val_loss: 4390.3188 - val_pred_loss: 89.7873 - val_kl_loss: 16.9215 - val_recon_loss: 4373.3970\n",
      "Epoch 14/300\n",
      "412/412 [==============================] - 138s 334ms/step - loss: 4328.0664 - pred_loss: 92.3800 - kl_loss: 17.6324 - recon_loss: 4310.4360 - val_loss: 4387.0864 - val_pred_loss: 87.3827 - val_kl_loss: 16.8548 - val_recon_loss: 4370.2319\n",
      "Epoch 15/300\n",
      "412/412 [==============================] - 138s 335ms/step - loss: 4325.0776 - pred_loss: 90.9723 - kl_loss: 17.7486 - recon_loss: 4307.3325 - val_loss: 4387.9248 - val_pred_loss: 87.0894 - val_kl_loss: 17.0036 - val_recon_loss: 4370.9209\n",
      "Epoch 16/300\n",
      "412/412 [==============================] - 131s 317ms/step - loss: 4324.1021 - pred_loss: 91.0778 - kl_loss: 17.8643 - recon_loss: 4306.2373 - val_loss: 4388.1836 - val_pred_loss: 88.3607 - val_kl_loss: 17.0999 - val_recon_loss: 4371.0835\n",
      "Epoch 17/300\n",
      "412/412 [==============================] - 132s 321ms/step - loss: 4322.8979 - pred_loss: 90.7354 - kl_loss: 17.9378 - recon_loss: 4304.9614 - val_loss: 4388.7539 - val_pred_loss: 86.5820 - val_kl_loss: 17.1744 - val_recon_loss: 4371.5781\n",
      "Epoch 18/300\n",
      "412/412 [==============================] - ETA: 0s - loss: 4321.9966 - pred_loss: 90.7512 - kl_loss: 18.0200 - recon_loss: 4303.9810\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "412/412 [==============================] - 131s 317ms/step - loss: 4321.9966 - pred_loss: 90.7512 - kl_loss: 18.0200 - recon_loss: 4303.9810 - val_loss: 4389.1504 - val_pred_loss: 87.1465 - val_kl_loss: 17.2208 - val_recon_loss: 4371.9302\n",
      "Epoch 19/300\n",
      "412/412 [==============================] - 129s 314ms/step - loss: 4319.8604 - pred_loss: 90.7277 - kl_loss: 18.0586 - recon_loss: 4301.8057 - val_loss: 4389.2446 - val_pred_loss: 87.3411 - val_kl_loss: 17.2383 - val_recon_loss: 4372.0063\n",
      "Epoch 20/300\n",
      "412/412 [==============================] - 130s 316ms/step - loss: 4319.5693 - pred_loss: 90.8192 - kl_loss: 18.0703 - recon_loss: 4301.4980 - val_loss: 4389.2954 - val_pred_loss: 87.3386 - val_kl_loss: 17.2497 - val_recon_loss: 4372.0454\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "# no need to modify this block\n",
    "# train savercat model which do the dimension reduction\n",
    "loss = SAVER_net.model_finetune(adata, fit_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the low-dimensional embedding for all the cells, and save to a csv file\n",
    "meta_df_train = adata.obs\n",
    "z_train = SAVER_net.extra_models['mean_out'].predict([adata.X, adata.obsm['saver_batch']])\n",
    "z_df = pd.DataFrame(z_train, \n",
    "                    index = meta_df_train.index,\n",
    "                    columns = ['saver{}'.format(i+1) for i in range(SAVER_net.latent_k)])\n",
    "z_df.to_csv('../data/lowdim_savercat_allg.csv') # where you want to save the low-dimensional embeddings learned by SAVERCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
