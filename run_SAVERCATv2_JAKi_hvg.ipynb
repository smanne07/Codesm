{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Savercat with highly vairiable genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, LeakyReLU, Lambda\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\n",
      "1.5.1\n",
      "scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.20.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.1\n"
     ]
    }
   ],
   "source": [
    "base_name = os.path.basename(os.getcwd())\n",
    "print(base_name)\n",
    "print(sc.__version__)\n",
    "sc.settings.verbosity = 3  \n",
    "sc.logging.print_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 29259 × 2668\n",
      "    obs: 'Cycle', 'patient'\n"
     ]
    }
   ],
   "source": [
    "adata = sc.read_h5ad('../data/adata_subsample_hvg.h5ad')\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Savercat preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils functions in utils_0509.py\n",
    "from utils_0509 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_key = 'Cycle' # the name of the cell-level label to be predicted\n",
    "batch_key = 'patient' # the name of the cell-level label to be adjusted for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizing by total count per cell\n",
      "    finished (0:00:01): normalized adata.X and added    'n_counts', counts per cell before normalization (adata.obs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 29259 × 2668\n",
       "    obs: 'Cycle', 'patient', 'n_counts', 'size_factors'\n",
       "    var: 'mean', 'std'\n",
       "    uns: 'log1p'\n",
       "    obsm: 'saver_targetL', 'B_raw', 'B', 'loglib', 'saver_batch'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# savercat preprocessing step\n",
    "adata = savercat_preprocess(adata, predict_key=predict_key, adjust_key=batch_key)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import network buiding functions in network_0509.py\n",
    "from network_0509 import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if train on highly variable genes, then keep enc=(256, 256, 128), dec=(128, 256, 256)\n",
    "# leave all the parameters unchanged\n",
    "SAVER_net = CVAE(x_input_size = adata.n_vars, # number of genes\n",
    "                 b_input_size = adata.obsm['saver_batch'].shape[1], # number of batches including lib-size\n",
    "                 lb_input_size = adata.obsm['saver_targetL'].shape[1], # number of labels to predict\n",
    "                 enc = (256, 256, 128), # dim of the encoder\n",
    "                 dec = (128, 256, 256), # dim of the decoder\n",
    "                 latent_k = 30) # dimension of the low-dimensional latent space\n",
    "SAVER_net.build()\n",
    "SAVER_net.compile_model(pred_weight=1, kl_weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "412/412 [==============================] - 12s 30ms/step - loss: 812.0385 - pred_loss: 44.5037 - kl_loss: 13.4647 - recon_loss: 754.0351 - val_loss: 726.8876 - val_pred_loss: 31.4546 - val_kl_loss: 17.4698 - val_recon_loss: 677.8963\n",
      "Epoch 2/300\n",
      "412/412 [==============================] - 12s 30ms/step - loss: 711.2430 - pred_loss: 23.0290 - kl_loss: 11.3375 - recon_loss: 676.8601 - val_loss: 707.9414 - val_pred_loss: 25.3956 - val_kl_loss: 12.4498 - val_recon_loss: 670.0816\n",
      "Epoch 3/300\n",
      "412/412 [==============================] - 12s 30ms/step - loss: 695.8997 - pred_loss: 15.3168 - kl_loss: 10.5742 - recon_loss: 669.9968 - val_loss: 701.5477 - val_pred_loss: 24.5124 - val_kl_loss: 11.0520 - val_recon_loss: 665.9819\n",
      "Epoch 4/300\n",
      "412/412 [==============================] - ETA: 0s - loss: 687.2782 - pred_loss: 10.9851 - kl_loss: 10.3054 - recon_loss: 665.9832\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "412/412 [==============================] - 14s 34ms/step - loss: 687.2782 - pred_loss: 10.9851 - kl_loss: 10.3054 - recon_loss: 665.9832 - val_loss: 701.9214 - val_pred_loss: 28.5799 - val_kl_loss: 10.7108 - val_recon_loss: 662.6100\n",
      "Epoch 5/300\n",
      "411/412 [============================>.] - ETA: 0s - loss: 677.2400 - pred_loss: 5.7610 - kl_loss: 10.1829 - recon_loss: 661.2961\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 677.2034 - pred_loss: 5.7647 - kl_loss: 10.1829 - recon_loss: 661.2580 - val_loss: 696.8898 - val_pred_loss: 26.1238 - val_kl_loss: 9.9415 - val_recon_loss: 660.8081\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# no need to modify this block\n",
    "# label guided initialization step\n",
    "loss = SAVER_net.model_initialize(adata, fit_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the directory where you want to save the file\n",
    "# 'weights_step1.h5' is the file name\n",
    "SAVER_net.model.save_weights('../data/weights_init.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if train on highly variable genes, then keep enc=(256, 256, 128), dec=(128, 256, 256)\n",
    "# leave all the parameters unchanged\n",
    "# same as block 8 but use the weight you just saved\n",
    "SAVER_net = CVAE(x_input_size = adata.n_vars,\n",
    "                 b_input_size = adata.obsm['saver_batch'].shape[1],\n",
    "                 lb_input_size = adata.obsm['saver_targetL'].shape[1],\n",
    "                 enc = (256, 256, 128),\n",
    "                 dec = (128, 256, 256),\n",
    "                 latent_k = 30)\n",
    "SAVER_net.build()\n",
    "SAVER_net.load_weights('../data/weights_init.h5') # fill in the weight file you just saved\n",
    "SAVER_net.compile_model(pred_weight=0., kl_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 672.8130 - pred_loss: 15.3398 - kl_loss: 9.8709 - recon_loss: 662.9423 - val_loss: 670.8999 - val_pred_loss: 42.0698 - val_kl_loss: 10.6762 - val_recon_loss: 660.2236\n",
      "Epoch 2/300\n",
      "412/412 [==============================] - 12s 30ms/step - loss: 668.9038 - pred_loss: 32.3026 - kl_loss: 9.7614 - recon_loss: 659.1422 - val_loss: 667.9171 - val_pred_loss: 48.4594 - val_kl_loss: 9.8714 - val_recon_loss: 658.0456\n",
      "Epoch 3/300\n",
      "412/412 [==============================] - 14s 33ms/step - loss: 666.2770 - pred_loss: 41.5775 - kl_loss: 9.7577 - recon_loss: 656.5198 - val_loss: 665.9383 - val_pred_loss: 51.4322 - val_kl_loss: 10.0053 - val_recon_loss: 655.9332\n",
      "Epoch 4/300\n",
      "412/412 [==============================] - 14s 34ms/step - loss: 664.5171 - pred_loss: 46.4905 - kl_loss: 9.7208 - recon_loss: 654.7960 - val_loss: 664.3216 - val_pred_loss: 58.5913 - val_kl_loss: 9.6128 - val_recon_loss: 654.7088\n",
      "Epoch 5/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 662.6747 - pred_loss: 52.1315 - kl_loss: 9.7133 - recon_loss: 652.9612 - val_loss: 663.1118 - val_pred_loss: 57.9184 - val_kl_loss: 9.6292 - val_recon_loss: 653.4827\n",
      "Epoch 6/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 661.5025 - pred_loss: 56.3760 - kl_loss: 9.6692 - recon_loss: 651.8331 - val_loss: 662.0815 - val_pred_loss: 64.5029 - val_kl_loss: 9.4269 - val_recon_loss: 652.6546\n",
      "Epoch 7/300\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 660.2533 - pred_loss: 62.7214 - kl_loss: 9.6182 - recon_loss: 650.6352 - val_loss: 661.6628 - val_pred_loss: 67.9517 - val_kl_loss: 9.4462 - val_recon_loss: 652.2167\n",
      "Epoch 8/300\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 659.1193 - pred_loss: 68.2738 - kl_loss: 9.6107 - recon_loss: 649.5087 - val_loss: 661.0327 - val_pred_loss: 74.8203 - val_kl_loss: 9.5902 - val_recon_loss: 651.4426\n",
      "Epoch 9/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 658.2086 - pred_loss: 75.3008 - kl_loss: 9.6278 - recon_loss: 648.5805 - val_loss: 660.4957 - val_pred_loss: 74.4986 - val_kl_loss: 9.4546 - val_recon_loss: 651.0410\n",
      "Epoch 10/300\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 657.4335 - pred_loss: 78.6277 - kl_loss: 9.5934 - recon_loss: 647.8403 - val_loss: 660.2407 - val_pred_loss: 78.4774 - val_kl_loss: 9.4850 - val_recon_loss: 650.7557\n",
      "Epoch 11/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 656.8143 - pred_loss: 82.7349 - kl_loss: 9.6523 - recon_loss: 647.1615 - val_loss: 659.6722 - val_pred_loss: 88.6218 - val_kl_loss: 9.6375 - val_recon_loss: 650.0347\n",
      "Epoch 12/300\n",
      "412/412 [==============================] - 13s 33ms/step - loss: 656.1900 - pred_loss: 91.3290 - kl_loss: 9.7348 - recon_loss: 646.4553 - val_loss: 659.5757 - val_pred_loss: 91.5239 - val_kl_loss: 9.5470 - val_recon_loss: 650.0286\n",
      "Epoch 13/300\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 655.4980 - pred_loss: 97.4166 - kl_loss: 9.7531 - recon_loss: 645.7448 - val_loss: 659.3151 - val_pred_loss: 98.8921 - val_kl_loss: 9.5683 - val_recon_loss: 649.7468\n",
      "Epoch 14/300\n",
      "412/412 [==============================] - 13s 30ms/step - loss: 654.7939 - pred_loss: 101.4703 - kl_loss: 9.7596 - recon_loss: 645.0343 - val_loss: 658.5667 - val_pred_loss: 100.0802 - val_kl_loss: 9.5964 - val_recon_loss: 648.9702\n",
      "Epoch 15/300\n",
      "412/412 [==============================] - 13s 30ms/step - loss: 654.2790 - pred_loss: 105.8137 - kl_loss: 9.8539 - recon_loss: 644.4251 - val_loss: 658.2593 - val_pred_loss: 104.1818 - val_kl_loss: 9.6934 - val_recon_loss: 648.5660\n",
      "Epoch 16/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 653.6143 - pred_loss: 109.2020 - kl_loss: 9.8650 - recon_loss: 643.7498 - val_loss: 657.3912 - val_pred_loss: 110.9781 - val_kl_loss: 9.6818 - val_recon_loss: 647.7094\n",
      "Epoch 17/300\n",
      "412/412 [==============================] - 14s 34ms/step - loss: 653.1945 - pred_loss: 114.6526 - kl_loss: 9.9492 - recon_loss: 643.2460 - val_loss: 657.9401 - val_pred_loss: 115.5687 - val_kl_loss: 9.7731 - val_recon_loss: 648.1671\n",
      "Epoch 18/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 652.6492 - pred_loss: 119.7306 - kl_loss: 10.0126 - recon_loss: 642.6366 - val_loss: 657.8492 - val_pred_loss: 119.4642 - val_kl_loss: 9.8676 - val_recon_loss: 647.9817\n",
      "Epoch 19/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 652.2056 - pred_loss: 122.4739 - kl_loss: 10.0959 - recon_loss: 642.1098 - val_loss: 657.3425 - val_pred_loss: 121.6909 - val_kl_loss: 9.7668 - val_recon_loss: 647.5757\n",
      "Epoch 20/300\n",
      "412/412 [==============================] - 14s 33ms/step - loss: 651.7331 - pred_loss: 125.4307 - kl_loss: 10.1754 - recon_loss: 641.5577 - val_loss: 657.2128 - val_pred_loss: 124.5939 - val_kl_loss: 9.9324 - val_recon_loss: 647.2806\n",
      "Epoch 21/300\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 651.4327 - pred_loss: 127.4075 - kl_loss: 10.2702 - recon_loss: 641.1626 - val_loss: 657.0613 - val_pred_loss: 129.0472 - val_kl_loss: 10.1890 - val_recon_loss: 646.8722\n",
      "Epoch 22/300\n",
      "412/412 [==============================] - 14s 33ms/step - loss: 650.8245 - pred_loss: 128.8175 - kl_loss: 10.3417 - recon_loss: 640.4826 - val_loss: 656.8029 - val_pred_loss: 124.7045 - val_kl_loss: 10.1543 - val_recon_loss: 646.6486\n",
      "Epoch 23/300\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 650.4246 - pred_loss: 129.5338 - kl_loss: 10.4213 - recon_loss: 640.0033 - val_loss: 657.4243 - val_pred_loss: 124.2504 - val_kl_loss: 10.1838 - val_recon_loss: 647.2405\n",
      "Epoch 24/300\n",
      "412/412 [==============================] - 14s 33ms/step - loss: 650.1080 - pred_loss: 132.0738 - kl_loss: 10.4837 - recon_loss: 639.6243 - val_loss: 656.5799 - val_pred_loss: 130.7540 - val_kl_loss: 10.3498 - val_recon_loss: 646.2300\n",
      "Epoch 25/300\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 649.6379 - pred_loss: 133.2061 - kl_loss: 10.5834 - recon_loss: 639.0544 - val_loss: 656.3693 - val_pred_loss: 129.8643 - val_kl_loss: 10.2700 - val_recon_loss: 646.0993\n",
      "Epoch 26/300\n",
      "412/412 [==============================] - 12s 30ms/step - loss: 649.4181 - pred_loss: 134.9562 - kl_loss: 10.5924 - recon_loss: 638.8254 - val_loss: 656.6349 - val_pred_loss: 134.8361 - val_kl_loss: 10.3491 - val_recon_loss: 646.2858\n",
      "Epoch 27/300\n",
      "412/412 [==============================] - 14s 33ms/step - loss: 649.0312 - pred_loss: 134.5444 - kl_loss: 10.6357 - recon_loss: 638.3959 - val_loss: 656.1351 - val_pred_loss: 134.9393 - val_kl_loss: 10.4378 - val_recon_loss: 645.6973\n",
      "Epoch 28/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 648.6251 - pred_loss: 136.6783 - kl_loss: 10.7539 - recon_loss: 637.8711 - val_loss: 656.1672 - val_pred_loss: 133.4563 - val_kl_loss: 10.4666 - val_recon_loss: 645.7004\n",
      "Epoch 29/300\n",
      "412/412 [==============================] - 13s 33ms/step - loss: 648.3907 - pred_loss: 136.7400 - kl_loss: 10.7862 - recon_loss: 637.6047 - val_loss: 655.7625 - val_pred_loss: 131.2149 - val_kl_loss: 10.4954 - val_recon_loss: 645.2670\n",
      "Epoch 30/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 648.1398 - pred_loss: 137.2075 - kl_loss: 10.8207 - recon_loss: 637.3191 - val_loss: 655.6525 - val_pred_loss: 136.0209 - val_kl_loss: 10.6070 - val_recon_loss: 645.0455\n",
      "Epoch 31/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 647.9102 - pred_loss: 137.4425 - kl_loss: 10.8779 - recon_loss: 637.0319 - val_loss: 655.6409 - val_pred_loss: 135.2243 - val_kl_loss: 10.6328 - val_recon_loss: 645.0082\n",
      "Epoch 32/300\n",
      "412/412 [==============================] - 14s 34ms/step - loss: 647.2042 - pred_loss: 139.3759 - kl_loss: 10.8691 - recon_loss: 636.3350 - val_loss: 656.0104 - val_pred_loss: 134.3401 - val_kl_loss: 10.5490 - val_recon_loss: 645.4614\n",
      "Epoch 33/300\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 647.1589 - pred_loss: 139.7820 - kl_loss: 10.9051 - recon_loss: 636.2538 - val_loss: 655.2563 - val_pred_loss: 136.8396 - val_kl_loss: 10.6549 - val_recon_loss: 644.6013\n",
      "Epoch 34/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 14s 35ms/step - loss: 646.9056 - pred_loss: 139.8928 - kl_loss: 10.9849 - recon_loss: 635.9213 - val_loss: 655.2656 - val_pred_loss: 136.4519 - val_kl_loss: 10.7354 - val_recon_loss: 644.5303\n",
      "Epoch 35/300\n",
      "412/412 [==============================] - 14s 33ms/step - loss: 646.4771 - pred_loss: 139.5728 - kl_loss: 11.0205 - recon_loss: 635.4564 - val_loss: 655.6636 - val_pred_loss: 137.5453 - val_kl_loss: 10.8075 - val_recon_loss: 644.8561\n",
      "Epoch 36/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 646.4547 - pred_loss: 139.9367 - kl_loss: 11.0462 - recon_loss: 635.4081 - val_loss: 655.6312 - val_pred_loss: 133.9467 - val_kl_loss: 10.7918 - val_recon_loss: 644.8394\n",
      "Epoch 37/300\n",
      "411/412 [============================>.] - ETA: 0s - loss: 646.2880 - pred_loss: 140.7673 - kl_loss: 11.1212 - recon_loss: 635.1669\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "412/412 [==============================] - 13s 31ms/step - loss: 646.2560 - pred_loss: 140.5988 - kl_loss: 11.1211 - recon_loss: 635.1349 - val_loss: 655.3387 - val_pred_loss: 137.5155 - val_kl_loss: 10.8439 - val_recon_loss: 644.4949\n",
      "Epoch 38/300\n",
      "412/412 [==============================] - 13s 32ms/step - loss: 643.6612 - pred_loss: 140.8111 - kl_loss: 11.0941 - recon_loss: 632.5668 - val_loss: 654.1482 - val_pred_loss: 136.1174 - val_kl_loss: 10.8284 - val_recon_loss: 643.3197\n",
      "Epoch 39/300\n",
      "412/412 [==============================] - 14s 33ms/step - loss: 643.0015 - pred_loss: 139.5506 - kl_loss: 11.1027 - recon_loss: 631.8988 - val_loss: 654.0013 - val_pred_loss: 136.1533 - val_kl_loss: 10.8299 - val_recon_loss: 643.1715\n",
      "Epoch 40/300\n",
      "412/412 [==============================] - 14s 33ms/step - loss: 642.7899 - pred_loss: 139.7170 - kl_loss: 11.1082 - recon_loss: 631.6815 - val_loss: 653.7300 - val_pred_loss: 135.6033 - val_kl_loss: 10.8355 - val_recon_loss: 642.8946\n",
      "Epoch 41/300\n",
      "412/412 [==============================] - 13s 33ms/step - loss: 643.0775 - pred_loss: 140.1981 - kl_loss: 11.1255 - recon_loss: 631.9521 - val_loss: 653.6909 - val_pred_loss: 135.3997 - val_kl_loss: 10.8612 - val_recon_loss: 642.8298\n",
      "Epoch 42/300\n",
      "412/412 [==============================] - 12s 29ms/step - loss: 642.4148 - pred_loss: 140.3526 - kl_loss: 11.1338 - recon_loss: 631.2811 - val_loss: 653.9389 - val_pred_loss: 135.6589 - val_kl_loss: 10.8546 - val_recon_loss: 643.0844\n",
      "Epoch 43/300\n",
      "412/412 [==============================] - 12s 29ms/step - loss: 642.4181 - pred_loss: 140.2553 - kl_loss: 11.1416 - recon_loss: 631.2764 - val_loss: 653.7518 - val_pred_loss: 135.2531 - val_kl_loss: 10.8601 - val_recon_loss: 642.8915\n",
      "Epoch 44/300\n",
      "412/412 [==============================] - 12s 30ms/step - loss: 642.4217 - pred_loss: 139.8273 - kl_loss: 11.1581 - recon_loss: 631.2637 - val_loss: 654.0332 - val_pred_loss: 135.1509 - val_kl_loss: 10.8512 - val_recon_loss: 643.1821\n",
      "Epoch 45/300\n",
      "411/412 [============================>.] - ETA: 0s - loss: 642.4617 - pred_loss: 139.6926 - kl_loss: 11.1702 - recon_loss: 631.2913\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "412/412 [==============================] - 12s 29ms/step - loss: 642.4447 - pred_loss: 139.5025 - kl_loss: 11.1703 - recon_loss: 631.2740 - val_loss: 653.8433 - val_pred_loss: 135.0182 - val_kl_loss: 10.9109 - val_recon_loss: 642.9325\n",
      "Epoch 46/300\n",
      "412/412 [==============================] - 12s 30ms/step - loss: 641.8632 - pred_loss: 139.8467 - kl_loss: 11.1807 - recon_loss: 630.6825 - val_loss: 653.8099 - val_pred_loss: 135.4162 - val_kl_loss: 10.9087 - val_recon_loss: 642.9011\n",
      "Epoch 47/300\n",
      "412/412 [==============================] - 12s 29ms/step - loss: 641.9728 - pred_loss: 139.8942 - kl_loss: 11.1805 - recon_loss: 630.7920 - val_loss: 653.8644 - val_pred_loss: 135.4455 - val_kl_loss: 10.9128 - val_recon_loss: 642.9515\n",
      "Epoch 00047: early stopping\n"
     ]
    }
   ],
   "source": [
    "# no need to modify this block\n",
    "# train savercat model which do the dimension reduction\n",
    "loss = SAVER_net.model_finetune(adata, fit_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the low-dimensional embedding for all the cells, and save to a csv file\n",
    "meta_df_train = adata.obs\n",
    "z_train = SAVER_net.extra_models['mean_out'].predict([adata.X, adata.obsm['saver_batch']])\n",
    "z_df = pd.DataFrame(z_train, \n",
    "                    index = meta_df_train.index,\n",
    "                    columns = ['saver{}'.format(i+1) for i in range(SAVER_net.latent_k)])\n",
    "z_df.to_csv('../data/lowdim_savercat_hvg.csv') # where you want to save the low-dimensional embeddings learned by SAVERCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
